{"cells":[{"cell_type":"markdown","metadata":{"id":"ssSWUETlmJqF"},"source":["# Network Initialization"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"s2fRMtssl37b","executionInfo":{"status":"ok","timestamp":1758534013284,"user_tz":-120,"elapsed":7380,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"04a8b222-a2ca-4aac-8485-5dbd91fe6626"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install lark groq -q"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"fOd37di8lyWE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758534037581,"user_tz":-120,"elapsed":24282,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"0378a17c-63af-4839-d8b8-1c9b3c523664"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","from typing import Any, Callable, Literal\n","from datetime import datetime\n","from tqdm.auto import tqdm\n","from PIL import Image, ImageDraw, ImageFont\n","from sklearn.metrics import roc_auc_score\n","from groq import Groq\n","import os\n","import shutil\n","import io\n","import re\n","import json\n","import base64\n","import random\n","import inspect\n","import time\n","import itertools\n","import lark\n","import torch\n","import torchsummary\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","drive.mount('/content/drive')\n","DRIVE_ROOT_DIR_PATH = 'MyDrive/nesy'\n","DRIVE_DATASETS_DIR = 'prepared_datasets'\n","DRIVE_LOGS_DIR = 'logs'\n","GROQ_API_KEY = '<YOUR_GROQ_API_KEY>'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":29329,"status":"ok","timestamp":1758534066908,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"},"user_tz":-120},"id":"YeIF_HiJmcAk"},"outputs":[],"source":["if not os.path.exists('/content/data/'):\n","    os.makedirs('/content/data/')\n","for file_name in os.listdir(f'/content/drive/{DRIVE_ROOT_DIR_PATH}/{DRIVE_DATASETS_DIR}/'):\n","    if file_name.endswith('.pt'):\n","        shutil.copy(f'/content/drive/{DRIVE_ROOT_DIR_PATH}/{DRIVE_DATASETS_DIR}/{file_name}', '/content/data/')"]},{"cell_type":"markdown","metadata":{"id":"tld0b4TkAp4x"},"source":["# D-LTN"]},{"cell_type":"markdown","metadata":{"id":"rEHwVrOoAtAR"},"source":["## FOL Grammar"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0TWedeN_A5QZ","executionInfo":{"status":"ok","timestamp":1758534066916,"user_tz":-120,"elapsed":3,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"outputs":[],"source":["CONSTANT_TERMINAL = 'constant'\n","VARIABLE_TERMINAL = 'variable'\n","FUNCTION_TERMINAL = 'function'\n","PREDICATE_TERMINAL = 'predicate'\n","WRAPPER_TERMINAL = 'wrapper'\n","LOGICAL_NOT_TERMINAL = 'logical_not'\n","LOGICAL_AND_TERMINAL = 'logical_and'\n","LOGICAL_OR_TERMINAL = 'logical_or'\n","IMPLIES_TERMINAL = 'implies'\n","IFF_TERMINAL = 'iff'\n","FORALL_TERMINAL = 'forall'\n","EXISTS_TERMINAL = 'exists'\n","\n","CONSTANT_SYMBOL = 'C'\n","VARIABLE_SYMBOL = 'x'\n","FUNCTION_SYMBOL = 'f'\n","PREDICATE_SYMBOL = 'P'\n","WRAPPER_SYMBOL = ''\n","LOGICAL_NOT_SYMBOL = '!'\n","LOGICAL_AND_SYMBOL = '&'\n","LOGICAL_OR_SYMBOL = '|'\n","IMPLIES_SYMBOL = 'implies'\n","IFF_SYMBOL = 'iff'\n","FORALL_SYMBOL = 'forall'\n","EXISTS_SYMBOL = 'exists'\n","\n","FOL_GRAMMAR = f'''\n","//// Explanations ////\n","\n","    // {CONSTANT_TERMINAL} identifiers always start with \"{CONSTANT_SYMBOL}\"\n","    // {VARIABLE_TERMINAL} identifiers always start with \"{VARIABLE_SYMBOL}\"\n","    // {FUNCTION_TERMINAL} identifiers always start with \"{FUNCTION_SYMBOL}\"\n","    // {PREDICATE_TERMINAL} identifiers always start with \"{PREDICATE_SYMBOL}\"\n","    // wrapper symbol is \"{WRAPPER_SYMBOL}(\" and \")\"\n","    // negation symbol is \"{LOGICAL_NOT_SYMBOL}\"\n","    // conjunction symbol is \"{LOGICAL_AND_SYMBOL}\"\n","    // disjunction symbol is \"{LOGICAL_OR_SYMBOL}\"\n","    // implication symbol is \"{IMPLIES_SYMBOL}\"\n","    // equivalence symbol is \"{IFF_SYMBOL}\"\n","    // universal quantifier symbol is \"{FORALL_SYMBOL}\"\n","    // existential quantifier symbol is \"{EXISTS_SYMBOL}\"\n","\n","//// Initialization ////\n","\n","    // imports\n","     %import common.WS\n","     %ignore WS\n","\n","    // entry point\n","    ?start: expression\n","\n","//// Term-Level Terminal Definitions  ////\n","\n","    // Tree Structure:\n","    // term\n","    // ├─atom\n","    // │ └─{CONSTANT_TERMINAL}, {VARIABLE_TERMINAL}\n","    // └─mapper\n","    //   └─{FUNCTION_TERMINAL}\n","\n","    // Abstract Terminal (no precedence)\n","    ?term: {CONSTANT_TERMINAL} | {VARIABLE_TERMINAL} | {FUNCTION_TERMINAL}\n","\n","    // Concrete Terminals (no precedence)\n","    {CONSTANT_TERMINAL}: /{CONSTANT_SYMBOL}[a-z0-9_]*/\n","    {VARIABLE_TERMINAL}: /{VARIABLE_SYMBOL}[a-z0-9_]*/\n","    {FUNCTION_TERMINAL}: /{FUNCTION_SYMBOL}[a-z0-9_]*/ \"(\" term (\",\" term)* \")\"\n","\n","//// Expression-Level Terminal Definitions ////\n","\n","    // Tree Structure:\n","    // expression\n","    // ├─evaluator\n","    // │ └─{PREDICATE_TERMINAL}\n","    // ├─unary_connective\n","    // │ └─{WRAPPER_TERMINAL}, {LOGICAL_NOT_TERMINAL}\n","    // ├─binary_connective\n","    // │ └─{LOGICAL_AND_TERMINAL}, {LOGICAL_OR_TERMINAL}, {IMPLIES_TERMINAL}, {IFF_TERMINAL}\n","    // └─quantifier\n","    //   └─{FORALL_TERMINAL}, {EXISTS_TERMINAL}\n","\n","    // Abstract Terminal (ascending precedence)\n","    ?expression: level_0\n","    ?level_0: level_1 | {EXISTS_TERMINAL} | {FORALL_TERMINAL}\n","    ?level_1: level_2 | {IFF_TERMINAL} | {IMPLIES_TERMINAL}\n","    ?level_2: level_3 | {LOGICAL_OR_TERMINAL}\n","    ?level_3: level_4 | {LOGICAL_AND_TERMINAL}\n","    ?level_4: level_5 | {LOGICAL_NOT_TERMINAL} | {WRAPPER_TERMINAL}\n","    ?level_5: predicate\n","\n","    // Concrete Terminals (ascending precedence)\n","    {PREDICATE_TERMINAL}: /{PREDICATE_SYMBOL}[a-z0-9_]*/ \"(\" term (\",\" term)* \")\"\n","    {WRAPPER_TERMINAL}: \"{WRAPPER_SYMBOL}(\" expression \")\"\n","    {LOGICAL_NOT_TERMINAL}: \"{LOGICAL_NOT_SYMBOL}\" level_4\n","    {LOGICAL_AND_TERMINAL}: level_4 \"{LOGICAL_AND_SYMBOL}\" level_3\n","    {LOGICAL_OR_TERMINAL}: level_2 \"{LOGICAL_OR_SYMBOL}\" level_3\n","    {IMPLIES_TERMINAL}: level_1 \"{IMPLIES_SYMBOL}\" level_2\n","    {IFF_TERMINAL}: level_1 \"{IFF_SYMBOL}\" level_2\n","    {FORALL_TERMINAL}: \"{FORALL_SYMBOL}\" {VARIABLE_TERMINAL} (\",\" {VARIABLE_TERMINAL})* expression\n","    {EXISTS_TERMINAL}: \"{EXISTS_SYMBOL}\" {VARIABLE_TERMINAL} (\",\" {VARIABLE_TERMINAL})* expression\n","'''"]},{"cell_type":"markdown","metadata":{"id":"aQ_mm5_FAvHI"},"source":["## Groundings"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"QY4IH2zDA_xx","executionInfo":{"status":"ok","timestamp":1758534066935,"user_tz":-120,"elapsed":16,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"outputs":[],"source":["class Grd():\n","\n","    # Abstract Super Grounding\n","    class Base():\n","        def __init__(self, description:str, content:None|str|torch.Tensor|torch.nn.Module|Callable[..., torch.Tensor], hyper_arg_dict:dict[str, int|float]) -> None:\n","            self.description = description\n","            self.content = content\n","            self.hyper_arg_dict = hyper_arg_dict\n","        def __repr__(self) -> str:\n","            return self.__str__()\n","        def __str__(self) -> str:\n","            return self.description\n","        def __call__(self, *args:Any) -> Any:\n","            raise NotImplementedError()\n","\n","    # Concrete Groundings\n","    class Empty(Base):\n","        def __init__(self) -> None:\n","            super().__init__('empty', None, {})\n","        def __call__(self) -> None:\n","            return self.content\n","\n","    class Command(Base):\n","        def __init__(self, command:str) -> None:\n","            super().__init__('command', command, {})\n","        def __call__(self) -> str:\n","            return self.content\n","\n","    class Value(Base):\n","        def __init__(self, tensor:torch.Tensor) -> None:\n","            shape = 'scalar' if tensor.ndim == 0 else f'{tensor.shape[0]}x0' if tensor.ndim == 1 else 'x'.join(str(dim) for dim in tensor.shape)\n","            trainability = 'trainable' if tensor.requires_grad else 'non-trainable'\n","            super().__init__(f'tensor[{shape}, {trainability}]', tensor, {})\n","        def __call__(self) -> torch.Tensor:\n","            return self.content\n","\n","    class Network(Base):\n","        def __init__(self, network:torch.nn.Module) -> None:\n","            tot_params = sum(tensor.numel() for tensor in network.parameters())\n","            tot_trainable_params = sum(tensor.numel() for tensor in network.parameters() if tensor.requires_grad)\n","            super().__init__(f'network[{tot_params} params, {tot_trainable_params} trainable]', network, {})\n","        def __call__(self, *args:torch.Tensor) -> torch.Tensor:\n","            return self.content(*args)\n","\n","    class Routine(Base):\n","        @classmethod\n","        def wrap(cls, description:str) -> Callable[[Callable[..., torch.Tensor]], 'Grd.Routine']:\n","            def decorator(routine: Callable[..., torch.Tensor]) -> Grd.Routine:\n","                return Grd.Routine(staticmethod(routine), description)\n","            return decorator\n","        def __init__(self, routine:Callable[..., torch.Tensor], description:str):\n","            signature = inspect.signature(routine).parameters\n","            assert all(arg_value.annotation is not inspect._empty for arg_value in signature.values()), \"The routine must be type-hinted!\"\n","            hyper_arg_dict = dict[str, int|float]()\n","            for arg_name, arg_value in signature.items():\n","                if arg_value.annotation in (int, float):\n","                    hyper_arg_dict[arg_name] = arg_value.default if arg_value.default is not inspect._empty else None\n","            if len(hyper_arg_dict) > 0:\n","                hyper_arg = f', {len(hyper_arg_dict)} hyper-arg' + ('s' if len(hyper_arg_dict) > 1 else '') + f', {sum(hyper_arg_value is None for hyper_arg_value in hyper_arg_dict.values())} empty'\n","            else:\n","                hyper_arg = ''\n","            super().__init__(f'{routine.__name__}[{description}{hyper_arg}]', routine, hyper_arg_dict)\n","        def __call__(self, *args:torch.Tensor) -> torch.Tensor:\n","            return self.content(*args, **self.hyper_arg_dict)"]},{"cell_type":"markdown","metadata":{"id":"NhkqnpjGAyJi"},"source":["## Blocks"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"45sakg6uBDah","executionInfo":{"status":"ok","timestamp":1758534067066,"user_tz":-120,"elapsed":31,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"outputs":[],"source":["class Blk():\n","\n","    # Abstract Super Block\n","    class Base():\n","        PRINT_PRIORITY = 0\n","        TERMINAL:str|None = None\n","        def __init__(self, name:str, symbol:str, *children:'Blk.Base') -> None:\n","            self.name = name\n","            self.symbol = symbol\n","            self.children = children\n","            self.current_grd:Grd.Base = Grd.Empty()\n","            self.default_grd_dict = {k: v for k, v in self.__class__.__dict__.items() if isinstance(v, Grd.Routine)}\n","            self.global_blk_list = list['Blk.Base']()\n","            self.global_domain_list = list[str]()\n","            self.involved_domain_set = set[str]()\n","            self.arg_shape_dict = dict[str, list[str]]()\n","            self.inp_shape_list = list[str]()\n","            self.out_shape_list = list[str]()\n","        def __repr__(self) -> str:\n","            return self.__str__()\n","        def __str__(self) -> str:\n","            raise NotImplementedError()\n","        def __call__(self, **value_dict:torch.Tensor) -> torch.Tensor:\n","            raise NotImplementedError()\n","        def setup_blks(self, global_blk_list:list['Blk.Base']) -> None:\n","            for child in self.children:\n","                child.setup_blks(global_blk_list)\n","            global_blk_list.append(self)\n","            self.global_blk_list = global_blk_list\n","        def setup_domains(self, global_domain_list:list[str]) -> None:\n","            for child in self.children:\n","                child.setup_domains(global_domain_list)\n","            if isinstance(self, Blk.Quantifier):\n","                for variable in self.children[:-1]:\n","                    if variable.name not in global_domain_list:\n","                        global_domain_list.append(variable.name)\n","            global_domain_list.sort()\n","            self.global_domain_list = global_domain_list\n","        def setup_shapes(self) -> None:\n","            for child in self.children:\n","                child.setup_shapes()\n","            if isinstance(self, Blk.Atom):\n","                self.out_shape_list.append('B*')\n","                if self.name in self.global_domain_list:\n","                    self.involved_domain_set = {self.name}\n","                    self.out_shape_list.append(f'D{self.name}')\n","                self.out_shape_list.append(f'E{self.name}')\n","            elif isinstance(self, Blk.Mapper):\n","                for term in self.children:\n","                    self.involved_domain_set.update(term.involved_domain_set)\n","                    self.arg_shape_dict[term.name] = term.out_shape_list\n","                self.out_shape_list.append('B*')\n","                for domain in self.global_domain_list:\n","                    self.out_shape_list.append(f'D{domain}' if domain in self.involved_domain_set else '1')\n","                self.out_shape_list.append(f'E{self.name}')\n","            elif isinstance(self, Blk.BinaryConnective):\n","                left, right = self.children\n","                self.involved_domain_set.update(left.involved_domain_set)\n","                self.involved_domain_set.update(right.involved_domain_set)\n","                self.arg_shape_dict['left_tensor'] = ['B*'] + [f'D{domain}*' for domain in self.global_domain_list]\n","                self.arg_shape_dict['right_tensor'] = ['B*'] + [f'D{domain}*' for domain in self.global_domain_list]\n","                self.out_shape_list = ['B*'] + [f'D{domain}*' for domain in self.global_domain_list]\n","            elif isinstance(self, Blk.UnaryConnective):\n","                body = self.children[0]\n","                self.involved_domain_set.update(body.involved_domain_set)\n","                self.arg_shape_dict['tensor'] = ['B*'] + [f'D{domain}*' for domain in self.global_domain_list]\n","                self.out_shape_list = ['B*'] + [f'D{domain}*' for domain in self.global_domain_list]\n","            elif isinstance(self, Blk.Quantifier):\n","                body = self.children[-1]\n","                self.involved_domain_set.update(self.global_domain_list)\n","                for variable in self.children[:-1]:\n","                    self.involved_domain_set.remove(variable.name)\n","                self.arg_shape_dict['tensor'] = ['B*'] + [f'D{domain}*' for domain in self.global_domain_list]\n","                self.out_shape_list = ['B*'] + [f'D{domain}**' for domain in self.global_domain_list]\n","            elif isinstance(self, Blk.Evaluator):\n","                for term in self.children:\n","                    self.involved_domain_set.update(term.involved_domain_set)\n","                    self.arg_shape_dict[term.name] = term.out_shape_list\n","                self.out_shape_list.append('B*')\n","                for domain in self.global_domain_list:\n","                    self.out_shape_list.append(f'D{domain}' if domain in self.involved_domain_set else '1')\n","        def ground(self, **context_dict:Grd.Command|Grd.Value|Grd.Network|Grd.Routine) -> None:\n","            if self.name in context_dict:\n","                temporary_ground = context_dict[self.name]\n","                if isinstance(temporary_ground, Grd.Command):\n","                    self.current_grd = self.default_grd_dict[temporary_ground()]\n","                else:\n","                    self.current_grd = temporary_ground\n","            for child in self.children:\n","                child.ground(**context_dict)\n","        def schedule(self, **hyper_arg_dict:dict[str, int|float]) -> None:\n","            if self.name in hyper_arg_dict:\n","                for arg_name, arg_value in hyper_arg_dict[self.name].items():\n","                    if arg_name in self.current_grd.hyper_arg_dict:\n","                        self.current_grd.hyper_arg_dict[arg_name] = arg_value\n","            for child in self.children:\n","                child.schedule(**hyper_arg_dict)\n","\n","    # Abstract High-Level Blocks\n","    class Term(Base):\n","        def __init__(self, name:str, symbol:str, *children:'Blk.Base') -> None:\n","            super().__init__(name, symbol, *children)\n","\n","    class Expression(Base):\n","        def __init__(self, name:str, symbol:str, *children:'Blk.Base') -> None:\n","            super().__init__(name, symbol, *children)\n","\n","    # Abstract Medium-Level Blocks\n","    class Atom(Term):\n","        PRINT_PRIORITY = 1\n","        def __init__(self, name:str, symbol:str) -> None:\n","            super().__init__(name, symbol)\n","            self.current_grd:Grd.Empty|Grd.Value = Grd.Empty()\n","        def __str__(self) -> str:\n","            return self.name\n","        def __call__(self, **value_dict:torch.Tensor) -> torch.Tensor:\n","            if isinstance(self.current_grd, Grd.Empty):\n","                return value_dict[self.name]\n","            return self.current_grd()\n","\n","    class Mapper(Term):\n","        PRINT_PRIORITY = 2\n","        def __init__(self, name:str, symbol:str, *terms:'Blk.Term') -> None:\n","            super().__init__(name, symbol, *terms)\n","            self.current_grd:Grd.Empty|Grd.Network|Grd.Routine = Grd.Empty()\n","        def __str__(self) -> str:\n","            return self.name + '(' + ', '.join(str(child) for child in self.children) + ')'\n","        def __call__(self, **value_dict:torch.Tensor) -> torch.Tensor:\n","            return self.current_grd(*[term(**value_dict) for term in self.children])\n","\n","    class Evaluator(Expression):\n","        PRINT_PRIORITY = 3\n","        def __init__(self, name:str, symbol:str, *terms:'Blk.Term') -> None:\n","            super().__init__(name, symbol, *terms)\n","            self.current_grd:Grd.Empty|Grd.Network|Grd.Routine = Grd.Empty()\n","        def __str__(self) -> str:\n","            return self.name + '(' + ', '.join(str(child) for child in self.children) + ')'\n","        def __call__(self, **value_dict:torch.Tensor) -> torch.Tensor:\n","            return self.current_grd(*[term(**value_dict) for term in self.children])\n","\n","    class UnaryConnective(Expression):\n","        PRINT_PRIORITY = 4\n","        def __init__(self, name:str, symbol:str, default_grd:Grd.Routine, body:'Blk.Expression') -> None:\n","            super().__init__(name, symbol, body)\n","            self.current_grd = default_grd\n","        def __str__(self) -> str:\n","            return self.symbol + '(' + str(self.children[0]) + ')'\n","        def __call__(self, **value_dict:torch.Tensor) -> torch.Tensor:\n","            body = self.children[0]\n","            return self.current_grd(body(**value_dict))\n","\n","    class BinaryConnective(Expression):\n","        PRINT_PRIORITY = 5\n","        def __init__(self, name:str, symbol:str, default_grd:Grd.Routine, left:'Blk.Expression', right:'Blk.Expression') -> None:\n","            super().__init__(name, symbol, left, right)\n","            self.current_grd = default_grd\n","        def __str__(self) -> str:\n","            return str(self.children[0]) + ' ' + self.symbol + ' ' + str(self.children[1])\n","        def __call__(self, **value_dict:torch.Tensor) -> torch.Tensor:\n","            left, right = self.children\n","            return self.current_grd(left(**value_dict), right(**value_dict))\n","\n","    class Quantifier(Expression):\n","        PRINT_PRIORITY = 6\n","        def __init__(self, name:str, symbol:str, default_grd:Grd.Routine, body:'Blk.Expression', *variables:'Blk.Variable') -> None:\n","            super().__init__(name, symbol, *variables, body)\n","            self.current_grd = default_grd\n","        def __str__(self) -> str:\n","            return self.symbol + '[' + ', '.join(str(child) for child in self.children[:-1]) + '](' + str(self.children[-1]) + ')'\n","        def __call__(self, **value_dict:torch.Tensor) -> torch.Tensor:\n","            variable_list = self.children[:-1]\n","            body = self.children[-1]\n","            reduction_dim_list = [self.global_domain_list.index(variable.name) + 1 for variable in variable_list]\n","            return self.current_grd(reduction_dim_list, body(**value_dict))\n","\n","    # Concrete Blocks\n","    class Constant(Atom):\n","        TERMINAL = CONSTANT_TERMINAL\n","        def __init__(self, in_list:list[lark.Token]) -> None:\n","            super().__init__(in_list[0].value, CONSTANT_SYMBOL)\n","\n","    class Variable(Atom):\n","        TERMINAL = VARIABLE_TERMINAL\n","        def __init__(self, in_list:list[lark.Token]) -> None:\n","            super().__init__(in_list[0].value, VARIABLE_SYMBOL)\n","\n","    class Function(Mapper):\n","        TERMINAL = FUNCTION_TERMINAL\n","        def __init__(self, in_list:list['lark.Token|Blk.Term']) -> None:\n","            super().__init__(in_list[0].value, FUNCTION_SYMBOL, *in_list[1:])\n","\n","    class Predicate(Evaluator):\n","        TERMINAL = PREDICATE_TERMINAL\n","        def __init__(self, in_list:list['lark.Token|Blk.Term']) -> None:\n","            super().__init__(in_list[0].value, PREDICATE_SYMBOL, *in_list[1:])\n","\n","    class Wrapper(UnaryConnective):\n","        TERMINAL = WRAPPER_TERMINAL\n","        def __init__(self, in_list:list['Blk.Expression']) -> None:\n","            super().__init__(WRAPPER_TERMINAL, WRAPPER_SYMBOL, self.identity, in_list[0])\n","        @Grd.Routine.wrap('x')\n","        def identity(tensor:torch.Tensor) -> torch.Tensor:\n","            return tensor\n","\n","    class LogicalNot(UnaryConnective):\n","        TERMINAL = LOGICAL_NOT_TERMINAL\n","        def __init__(self, in_list:list['Blk.Expression']) -> None:\n","            super().__init__(LOGICAL_NOT_TERMINAL, LOGICAL_NOT_SYMBOL, self.complementation, in_list[0])\n","        @Grd.Routine.wrap('1 - x')\n","        def complementation(tensor:torch.Tensor) -> torch.Tensor:\n","            return 1.0 - tensor\n","\n","    class LogicalAnd(BinaryConnective):\n","        TERMINAL = LOGICAL_AND_TERMINAL\n","        def __init__(self, in_list:list['Blk.Expression']) -> None:\n","            super().__init__(LOGICAL_AND_TERMINAL, LOGICAL_AND_SYMBOL, self.lukasiewicz, in_list[0], in_list[1])\n","        @Grd.Routine.wrap('max(0, x1 + x2 - 1)')\n","        def lukasiewicz(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.maximum(torch.tensor(0.0, device=left_tensor.device), left_tensor + right_tensor - 1.0)\n","        @Grd.Routine.wrap('min(x1, x2)')\n","        def godel(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.minimum(left_tensor, right_tensor)\n","        @Grd.Routine.wrap('x1 * x2')\n","        def goguen(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return left_tensor * right_tensor\n","\n","    class LogicalOr(BinaryConnective):\n","        TERMINAL = LOGICAL_OR_TERMINAL\n","        def __init__(self, in_list:list[Any]) -> None:\n","            super().__init__(LOGICAL_OR_TERMINAL, LOGICAL_OR_SYMBOL, self.lukasiewicz, in_list[0], in_list[1])\n","        @Grd.Routine.wrap('min(1, x1 + x2)')\n","        def lukasiewicz(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.minimum(torch.tensor(1.0, device=left_tensor.device), left_tensor + right_tensor)\n","        @Grd.Routine.wrap('max(x1, x2)')\n","        def godel(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.maximum(left_tensor, right_tensor)\n","        @Grd.Routine.wrap('1 - (1 - x1) * (1 - x2)')\n","        def goguen(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return 1.0 - (1.0 - left_tensor) * (1.0 - right_tensor)\n","\n","    class Implies(BinaryConnective):\n","        TERMINAL = IMPLIES_TERMINAL\n","        def __init__(self, in_list:list['Blk.Expression']) -> None:\n","            super().__init__(IMPLIES_TERMINAL, IMPLIES_SYMBOL, self.lukasiewicz, in_list[0], in_list[1])\n","        @Grd.Routine.wrap('min(1, 1 - x1 + x2)')\n","        def lukasiewicz(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.minimum(torch.tensor(1.0, device=left_tensor.device), 1 - left_tensor + right_tensor)\n","        @Grd.Routine.wrap('1 if x1 <= x2 else x2')\n","        def godel(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.where(left_tensor <= right_tensor, torch.tensor(1.0, device=left_tensor.device), right_tensor)\n","        @Grd.Routine.wrap('1 if x1 <=x2 else x2 / x1')\n","        def goguen(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.where(left_tensor <= right_tensor, torch.tensor(1.0, device=left_tensor.device), right_tensor / torch.clamp(left_tensor, min=1e-6))\n","        @Grd.Routine.wrap('max(1 - x1, x2)')\n","        def kleene_dienes(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.maximum(1 - left_tensor, right_tensor)\n","        @Grd.Routine.wrap('1 - x1 + x1 * x2')\n","        def reichenbach(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return 1.0 - left_tensor + left_tensor * right_tensor\n","\n","    class Iff(BinaryConnective):\n","        TERMINAL = IFF_TERMINAL\n","        def __init__(self, in_list:list['Blk.Expression']) -> None:\n","            super().__init__(IFF_TERMINAL, IFF_SYMBOL, self.linear_similarity, in_list[0], in_list[1])\n","        @Grd.Routine.wrap('1 - |x1 - x2|')\n","        def linear_similarity(left_tensor:torch.Tensor, right_tensor:torch.Tensor) -> torch.Tensor:\n","            return 1.0 - torch.abs(left_tensor - right_tensor)\n","\n","    class ForAll(Quantifier):\n","        TERMINAL = FORALL_TERMINAL\n","        def __init__(self, in_list:list['Blk.Variable|Blk.Expression']) -> None:\n","            super().__init__(FORALL_TERMINAL, FORALL_SYMBOL, self.lukasiewicz, in_list[-1], *in_list[:-1])\n","        @Grd.Routine.wrap('max(0, sum(xi) - n + 1)')\n","        def lukasiewicz(reduction_dim_list:list[int], tensor:torch.Tensor) -> torch.Tensor:\n","            sum_tensor = torch.sum(tensor, dim=reduction_dim_list, keepdim=True)\n","            n = torch.prod(torch.tensor([tensor.size(dim) for dim in reduction_dim_list], device=tensor.device))\n","            return torch.maximum(torch.tensor(0.0, device=tensor.device), sum_tensor - n + 1)\n","        @Grd.Routine.wrap('min(xi)')\n","        def godel(reduction_dim_list:list[int], tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.amin(tensor, dim=reduction_dim_list, keepdim=True)\n","        @Grd.Routine.wrap('prod(xi)')\n","        def goguen(reduction_dim_list:list[int], tensor:torch.Tensor) -> torch.Tensor:\n","            prod_tensor = tensor\n","            for dim in sorted(reduction_dim_list, reverse=True):\n","                prod_tensor = torch.prod(prod_tensor, dim=dim, keepdim=True)\n","            return prod_tensor\n","        @Grd.Routine.wrap('1 - mean((1 - xi) ** p) ** (1 / p)')\n","        def power_mean(reduction_dim_list:list[int], tensor:torch.Tensor, eps:float=0.1, p:int=2) -> torch.Tensor:\n","            return 1.0 - torch.mean((1.0 - (1 - eps) * tensor) ** p, dim=reduction_dim_list, keepdim=True) ** (1 / p)\n","\n","    class Exists(Quantifier):\n","        TERMINAL = EXISTS_TERMINAL\n","        def __init__(self, in_list:list['Blk.Variable|Blk.Expression']) -> None:\n","            super().__init__(EXISTS_TERMINAL, EXISTS_SYMBOL, self.lukasiewicz, in_list[-1], *in_list[:-1])\n","        @Grd.Routine.wrap('min(1, sum(xi))')\n","        def lukasiewicz(reduction_dim_list:list[int], tensor:torch.Tensor) -> torch.Tensor:\n","            sum_tensor = torch.sum(tensor, dim=reduction_dim_list, keepdim=True)\n","            return torch.minimum(torch.tensor(1.0, device=tensor.device), sum_tensor)\n","        @Grd.Routine.wrap('max(xi)')\n","        def godel(reduction_dim_list:list[int], tensor:torch.Tensor) -> torch.Tensor:\n","            return torch.amax(tensor, dim=reduction_dim_list, keepdim=True)\n","        @Grd.Routine.wrap('1 - prod(1 - xi)')\n","        def goguen(reduction_dim_list:list[int], tensor:torch.Tensor) -> torch.Tensor:\n","            prod_tensor = 1.0 - tensor\n","            for dim in sorted(reduction_dim_list, reverse=True):\n","                prod_tensor = torch.prod(prod_tensor, dim=dim, keepdim=True)\n","            return 1.0 - prod_tensor\n","        @Grd.Routine.wrap('mean(xi ** p) ** (1 / p)')\n","        def power_mean(reduction_dim_list:list[int], tensor:torch.Tensor, eps:float=0.1, p:int=1) -> torch.Tensor:\n","            return torch.mean(((1 - eps) *tensor + eps) ** p, dim=reduction_dim_list, keepdim=True) ** (1 / p)"]},{"cell_type":"markdown","metadata":{"id":"nDn9UkBYA1jx"},"source":["## Builder"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0YkkpeeqA1Ba","executionInfo":{"status":"ok","timestamp":1758534067108,"user_tz":-120,"elapsed":36,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"outputs":[],"source":["class LTNBuilder():\n","\n","    # Explicit Formula\n","    class Formula():\n","        def __init__(self, fol_rule:str, fol_tree:lark.Tree, ltn_expression:'Blk.Expression') -> None:\n","            self._fol_rule = fol_rule\n","            self._fol_tree = fol_tree\n","            self._ltn_expression = ltn_expression\n","        def __call__(self, **value_dict:torch.Tensor) -> torch.Tensor:\n","            output = self._ltn_expression(**value_dict).squeeze()\n","            if output.ndim == 0:\n","                output = output.unsqueeze(0)\n","            return output\n","        def get_parameters(self) -> list[torch.nn.Parameter]:\n","            parameter_list = list[torch.nn.Parameter]()\n","            for blk in self._ltn_expression.global_blk_list:\n","                if isinstance(blk.current_grd, Grd.Value):\n","                    parameter_list.append(blk.current_grd.content)\n","                elif isinstance(blk.current_grd, Grd.Network):\n","                    parameter_list += list(blk.current_grd.content.parameters())\n","            return parameter_list\n","        def set_config(self, mode:Literal['device', 'status'], param:str) -> None:\n","            if mode == 'device':\n","                for blk in self._ltn_expression.global_blk_list:\n","                    if isinstance(blk.current_grd, Grd.Value):\n","                        blk.current_grd.content = blk.current_grd.content.to(param)\n","                    elif isinstance(blk.current_grd, Grd.Network):\n","                        blk.current_grd.content = blk.current_grd.content.to(param)\n","            elif mode == 'status':\n","                if param == 'train':\n","                    for blk in self._ltn_expression.global_blk_list:\n","                        if isinstance(blk.current_grd, Grd.Value):\n","                            blk.current_grd.content.requires_grad = True\n","                        elif isinstance(blk.current_grd, Grd.Network):\n","                            blk.current_grd.content.train()\n","                elif param == 'eval':\n","                    for blk in self._ltn_expression.global_blk_list:\n","                        if isinstance(blk.current_grd, Grd.Value):\n","                            blk.current_grd.content.requires_grad = False\n","                        elif isinstance(blk.current_grd, Grd.Network):\n","                            blk.current_grd.content.eval()\n","        def ground(self, **raw_context_dict:str|torch.Tensor|torch.nn.Module|Callable[..., torch.Tensor]) -> None:\n","            context_dict = dict[str, Grd.Command|Grd.Value|Grd.Network|Grd.Routine]()\n","            for blk_name in raw_context_dict:\n","                if isinstance(raw_context_dict[blk_name], str):\n","                    context_dict[blk_name] = Grd.Command(raw_context_dict[blk_name])\n","                elif isinstance(raw_context_dict[blk_name], torch.Tensor):\n","                    context_dict[blk_name] = Grd.Value(raw_context_dict[blk_name])\n","                elif isinstance(raw_context_dict[blk_name], torch.nn.Module):\n","                    context_dict[blk_name] = Grd.Network(raw_context_dict[blk_name])\n","                elif isinstance(raw_context_dict[blk_name], Callable):\n","                    context_dict[blk_name] = Grd.Routine(raw_context_dict[blk_name], 'n/a')\n","            self._ltn_expression.ground(**context_dict)\n","        def schedule(self, **hyper_arg_dict:dict[str, int|float]) -> None:\n","            self._ltn_expression.schedule(**hyper_arg_dict)\n","        def get(self, mode:Literal['raw', 'parsed', 'tree', 'info']) -> str:\n","            output = None\n","            if mode == 'raw':\n","                output = self._fol_rule\n","            elif mode == 'parsed':\n","                output = self._ltn_expression\n","            elif mode == 'tree':\n","                output = self._fol_tree.pretty()[:-1]\n","            elif mode == 'info':\n","                ungrounded_blk_dict = dict[tuple[int, str], str]()\n","                grounded_blk_dict = dict[tuple[int, str], str]()\n","                for blk in self._ltn_expression.global_blk_list:\n","                    if isinstance(blk.current_grd, Grd.Empty):\n","                        ungrounded_blk_dict[blk.PRINT_PRIORITY, blk.name] = f'  {blk.name} -> {blk.TERMINAL}'\n","                    else:\n","                        grounded_blk_dict[blk.PRINT_PRIORITY, blk.name] = f'  {blk.name} -> {blk.current_grd}'\n","                        if len(blk.default_grd_dict) > 0:\n","                            grounded_blk_dict[blk.PRINT_PRIORITY, blk.name] += '*, ' + ', '.join(str(grd) for grd in blk.default_grd_dict.values() if grd != blk.current_grd)\n","                ungrounded_blk_dict = dict(sorted(ungrounded_blk_dict.items(), key=lambda item: item[0]))\n","                grounded_blk_dict = dict(sorted(grounded_blk_dict.items(), key=lambda item: item[0]))\n","                output = ''\n","                if len(ungrounded_blk_dict) > 0:\n","                    output += '\\nUngrounded Symbols:\\n' + '\\n'.join(ungrounded_blk_dict.values())\n","                if len(grounded_blk_dict) > 0:\n","                    output += '\\nGrounded Symbols:\\n' + '\\n'.join(grounded_blk_dict.values())\n","                output = output[1:] if output.startswith('\\n') else output\n","                output += (\n","                    '\\nFootnotes:'\n","                    '\\n  * -> current grounding'\n","                )\n","            elif mode == 'shapes':\n","                blk_dict = dict[tuple[int, str], str]()\n","                for blk in self._ltn_expression.global_blk_list:\n","                    shapes = list[str]()\n","                    if len(blk.arg_shape_dict) > 0:\n","                        argument_shape_list = list[str]()\n","                        for arg in blk.arg_shape_dict:\n","                            argument_shape_list.append(f'{arg}[' + ', '.join(blk.arg_shape_dict[arg]) + ']')\n","                        shapes.append('  inp shape -> ' + ', '.join(argument_shape_list))\n","                    if len(blk.inp_shape_list) > 0:\n","                        shapes.append('  inp shape -> ' + ', '.join(blk.inp_shape_list))\n","                    if len(blk.out_shape_list) > 0:\n","                        shapes.append('  out shape -> ' + ', '.join(blk.out_shape_list))\n","                    blk_dict[blk.PRINT_PRIORITY, blk.name] = '\\n'.join(shapes)\n","                blk_dict = dict(sorted(blk_dict.items(), key=lambda item: item[0]))\n","                output = '\\n'.join(f'{blk_name}:\\n{blk_shapes}' for (_, blk_name), blk_shapes in blk_dict.items())\n","                output += (\n","                    '\\nFootnotes:'\n","                    '\\n  B -> batch size'\n","                    '\\n  Dx -> domain size of `x`'\n","                    '\\n  Ex -> extra dimensions of `x`'\n","                    '\\n  * -> the dimension can also be equal to `1` but not missing'\n","                    '\\n  ** -> the dimension is equal to `1` if its corresponding variable is involved in the quantifier'\n","                )\n","            return output\n","\n","    def __init__(self, fol_grammar=FOL_GRAMMAR) -> None:\n","        self._fol_parser = lark.Lark(fol_grammar)\n","        self._fol_transformer = lark.Transformer()\n","        for Block in Blk.__dict__.values():\n","            if isinstance(Block, type) and issubclass(Block, Blk.Base) and Block.TERMINAL is not None:\n","                setattr(self._fol_transformer, Block.TERMINAL, Block)\n","\n","    def make_formula(self, fol_rule:str) -> Formula:\n","        fol_tree = self._fol_parser.parse(fol_rule)\n","        ltn_expression:Blk.Expression = self._fol_transformer.transform(fol_tree)\n","        ltn_expression.setup_blks(list['Blk.Base']())\n","        ltn_expression.setup_domains(list[str]())\n","        ltn_expression.setup_shapes()\n","        ltn_formula = LTNBuilder.Formula(fol_rule, fol_tree, ltn_expression)\n","        return ltn_formula"]},{"cell_type":"markdown","metadata":{"id":"AcozSyi2mM30"},"source":["# Main Algorithms"]},{"cell_type":"markdown","metadata":{"id":"yKRXfcWdD7iC"},"source":["## Groundings Tools"]},{"cell_type":"code","source":["class EncoderNet(torch.nn.Module):\n","\n","    def __init__(self, device:str, symbol_size:int, board_dim:int, cnn_dims:tuple[int, ...], kernel_dims:tuple[int, ...], embed_dims:tuple[int, ...], drop_prob:float, use_softmax:bool) -> None:\n","\n","        # Initialization\n","        super().__init__()\n","        self.device = device\n","        self.symbol_size = symbol_size\n","        self.board_dim = board_dim\n","        self.cnn_dims = cnn_dims\n","        self.kernel_dims = kernel_dims\n","        self.embed_dims = embed_dims\n","        self.drop_prob = drop_prob\n","        self.use_softmax = use_softmax\n","\n","        # CNN\n","        cnn_list = torch.nn.ModuleList()\n","        last_cnn_dim = 1\n","        pooled_symbol_size = symbol_size\n","        for cnn_dim, kernel_dim in zip(cnn_dims, kernel_dims):\n","            cnn_list.append(torch.nn.Conv2d(in_channels=last_cnn_dim, out_channels=cnn_dim, kernel_size=kernel_dim))\n","            cnn_list.append(torch.nn.ReLU())\n","            cnn_list.append(torch.nn.GroupNorm(num_groups=2, num_channels=cnn_dim))\n","            cnn_list.append(torch.nn.MaxPool2d(kernel_size=2))\n","            last_cnn_dim = cnn_dim\n","            pooled_symbol_size = (pooled_symbol_size - kernel_dim + 1) // 2\n","        cnn_list += [torch.nn.Dropout2d(p=drop_prob)]\n","        self.cnn = torch.nn.Sequential(*cnn_list)\n","\n","        # Embedder\n","        embed_list = torch.nn.ModuleList()\n","        last_embed_dim = last_cnn_dim * pooled_symbol_size * pooled_symbol_size\n","        for i, embed_dim in enumerate(embed_dims):\n","            embed_list.append(torch.nn.Linear(in_features=last_embed_dim, out_features=embed_dim))\n","            embed_list.append(torch.nn.ReLU())\n","            embed_list.append(torch.nn.Dropout(p=drop_prob))\n","            last_embed_dim = embed_dim\n","        embed_list = embed_list[:-2] + ([torch.nn.Softmax(dim=-1)] if use_softmax else [])\n","        self.embed = torch.nn.Sequential(*embed_list)\n","\n","        # Device Management\n","        self.to(device)\n","\n","    def forward(self, symbols:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n","        '''\n","        inputs:\n","            symbols: Tensor[batch_size, board_dim, board_dim, symbol_size, symbol_size][cpu][torch.uint8]\n","        outputs:\n","            embeds: Tensor[batch_size, board_dim**2, last_embed_dim][device][torch.float32]\n","            indices: Tensor[batch_size, board_dim**2, 2][device][torch.float32]\n","        '''\n","\n","        # Preparation\n","        batch_size = symbols.shape[0]\n","        symbols = symbols.to(self.device).to(torch.float32) / 255 # Tensor[batch_size, board_dim**2, symbol_size, symbol_size][device][torch.float32]\n","        symbols = symbols.view(-1, 1, self.symbol_size, self.symbol_size) # Tensor[batch_size*board_dim**2, 1, symbol_size, symbol_size][device][torch.float32]\n","\n","        # Visual Features\n","        features = self.cnn(symbols) # Tensor[batch_size*board_dim**2, last_cnn_dim, pooled_symbol_size, pooled_symbol_size][device][torch.float32]\n","        features = features.view(batch_size, self.board_dim ** 2, -1) # Tensor[batch_size, board_dim**2, last_cnn_dim*pooled_symbol_size*pooled_symbol_size][device][torch.float32]\n","\n","        # Embeddings\n","        embeds = self.embed(features) # Tensor[batch_size, board_dim**2, last_embed_dim][device][torch.float32]\n","\n","        # Indices\n","        indices = torch.cartesian_prod(torch.arange(self.board_dim), torch.arange(self.board_dim)) # Tensor[board_dim, board_dim][cpu][torch.int64]\n","        indices = indices.expand(batch_size, -1, 2) # Tensor[batch_size, board_dim**2, 2][cpu][torch.int64]\n","        indices = indices.to(self.device).to(torch.float32) # Tensor[batch_size, board_dim**2, 2][device][torch.float32]\n","\n","        # Ouput\n","        return embeds, indices"],"metadata":{"id":"5R4-8jfFBZNL","executionInfo":{"status":"ok","timestamp":1758534067167,"user_tz":-120,"elapsed":39,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Ie5nzSqnCz6B","executionInfo":{"status":"ok","timestamp":1758534067197,"user_tz":-120,"elapsed":29,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"outputs":[],"source":["def binary_similarity(left_tensor:torch.Tensor, right_tensor:torch.Tensor, eps:float=0.1) -> torch.Tensor:\n","    '''\n","    inputs:\n","        left_tensor: Tensor[B, Dl, E][device][torch.float]\n","        right_tensor: Tensor[B, Dr, E][device][torch.float]\n","    outputs:\n","        output: Tensor[B, Dl, Dr][device][torch.float]\n","    footnotes:\n","        B: batch size, i.e, `batch_size`\n","        Dl: domain size of `left_tensor`\n","        Dr: domain size of `right_tensor`\n","        E: extra dimension of the tensors\n","    '''\n","    left_tensor = left_tensor.unsqueeze(2) # Tensor[B, Dl, 1, E][device][torch.float32]\n","    right_tensor = right_tensor.unsqueeze(1) # Tensor[B, 1, Dr, E][device][torch.float32]\n","    similarity = (left_tensor - right_tensor).abs().sum(dim=-1) # Tensor[B, Dl, Dr][device][torch.float32]\n","    similarity = torch.where(similarity > 0.0, torch.zeros_like(similarity) + eps, torch.ones_like(similarity) - eps) # Tensor[B, Dl, Dr][device][torch.float32]\n","    return similarity\n","\n","def jaccard_similarity(left_tensor:torch.Tensor, right_tensor:torch.Tensor, eps:float=0.1, p:int=2) -> torch.Tensor:\n","    '''\n","    inputs:\n","        left_tensor: Tensor[B, Dl, E][device][torch.float]\n","        right_tensor: Tensor[B, Dr, E][device][torch.float]\n","        exp: exponentiation power\n","        eps: stability parameter\n","    outputs:\n","        output: Tensor[B, Dl, Dr][device][torch.float]\n","    footnotes:\n","        B: batch size, i.e, `batch_size`\n","        Dl: domain size of `left_tensor`\n","        Dr: domain size of `right_tensor`\n","        E: extra dimension of the tensors\n","    '''\n","    left_tensor = left_tensor.unsqueeze(2) # Tensor[B, Dl, 1, E][device][torch.float32]\n","    right_tensor = right_tensor.unsqueeze(1) # Tensor[B, 1, Dr, E][device][torch.float32]\n","    left_norm = (left_tensor * left_tensor).sum(dim=-1) # Tensor[B, Dl, 1][device][torch.float32]\n","    right_norm = (right_tensor * right_tensor).sum(dim=-1) # Tensor[B, 1, Dr][device][torch.float32]\n","    dot_product = (left_tensor * right_tensor).sum(dim=-1) # Tensor[B, Dl, Dr][device][torch.float32]\n","    similarity = 2 * dot_product / (left_norm + right_norm + eps) # Tensor[B, Dl, Dr][device][torch.float32]\n","    similarity = torch.where((left_norm < eps) & (right_norm < eps), torch.ones_like(similarity), similarity) # Tensor[B, Dl, Dr][device][torch.float32]\n","    return similarity ** p\n","\n","def exponential_similarity(left_tensor:torch.Tensor, right_tensor:torch.Tensor, eps:float=0.1, p:int=2) -> torch.Tensor:\n","    '''\n","    inputs:\n","        left_tensor: Tensor[B, Dl, E][device][torch.float]\n","        right_tensor: Tensor[B, Dr, E][device][torch.float]\n","        k: norm order\n","    outputs:\n","        output: Tensor[B, Dl, Dr][device][torch.float]\n","    footnotes:\n","        B: batch size, i.e, `batch_size`\n","        Dl: domain size of `left_tensor`\n","        Dr: domain size of `right_tensor`\n","        E: extra dimension of the tensors\n","    '''\n","    left_tensor = left_tensor.unsqueeze(2) # Tensor[B, Dl, 1, E][device][torch.float32]\n","    right_tensor = right_tensor.unsqueeze(1) # Tensor[B, 1, Dr, E][device][torch.float32]\n","    distance = torch.nn.functional.pairwise_distance(left_tensor, right_tensor, p=p) # Tensor[B, Dl, Dr][device][torch.float32]\n","    similarity = torch.exp(-torch.relu(distance - eps)) # Tensor[B, Dl, Dr][device][torch.float32]\n","    return similarity.to(torch.float32)\n","\n","def row_similarity(x1:torch.Tensor, x2:torch.Tensor) -> torch.Tensor:\n","    _, _, E = x1.shape\n","    x1 = x1[:, :, (E - 2,)]\n","    x2 = x2[:, :, (E - 2,)]\n","    return binary_similarity(x1, x2)\n","\n","def col_similarity(x1:torch.Tensor, x2:torch.Tensor) -> torch.Tensor:\n","    _, _, E = x1.shape\n","    x1 = x1[:, :, (E - 1,)]\n","    x2 = x2[:, :, (E - 1,)]\n","    return binary_similarity(x1, x2)\n","\n","def block_similarity(x1:torch.Tensor, x2:torch.Tensor) -> torch.Tensor:\n","    _, D, E = x1.shape\n","    total_samples_per_side = int(D ** 0.25)\n","    x1 = x1[:, :, E - 2:] // total_samples_per_side\n","    x2 = x2[:, :, E - 2:] // total_samples_per_side\n","    return binary_similarity(x1, x2)\n","\n","def loc_similarity(x1:torch.Tensor, x2:torch.Tensor) -> torch.Tensor:\n","    _, _, E = x1.shape\n","    x1 = x1[:, :, E - 2:]\n","    x2 = x2[:, :, E - 2:]\n","    return binary_similarity(x1, x2)\n","\n","def digit_similarity(x1:torch.Tensor, x2:torch.Tensor) -> torch.Tensor:\n","    _, _, E = x1.shape\n","    x1 = x1[:, :, (0,)]\n","    x2 = x2[:, :, (0,)]\n","    return binary_similarity(x1, x2)\n","\n","def vector_similarity(x1:torch.Tensor, x2:torch.Tensor) -> torch.Tensor:\n","    _, _, E = x1.shape\n","    x1 = x1[:, :, :E - 2]\n","    x2 = x2[:, :, :E - 2]\n","    return exponential_similarity(x1, x2)"]},{"cell_type":"code","source":["def original_power_mean_scheduler(epoch:int) -> int:\n","    if epoch < 20:\n","        return 1\n","    if epoch < 120:\n","        return 2\n","    if epoch < 140:\n","        return 4\n","    if epoch < 170:\n","        return 6\n","    if epoch < 200:\n","        return 8\n","    return 10\n","\n","def new_power_mean_scheduler(epoch:int) -> int:\n","    return int(5 + 5 * (epoch / 45))"],"metadata":{"id":"i1fX1DMmWkRI","executionInfo":{"status":"ok","timestamp":1758534067213,"user_tz":-120,"elapsed":3,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vQREwYvNBKu"},"source":["## Operation Tools"]},{"cell_type":"code","source":["class Logger():\n","\n","    def __init__(self, family:str) -> None:\n","        self.family_path = self._get_family_path(family)\n","        self.current_meta_timestamp:str = None\n","\n","    def dump_meta(self, reset:bool, **kwargs:dict[str, str]) -> None:\n","        if reset:\n","            self.current_meta_timestamp = datetime.now().strftime('%Y-%m-%d %H-%M-%S.%f')\n","        meta_path = os.path.join(self.family_path, 'meta.csv')\n","        self._update_df(reset, meta_path, timestamp=self.current_meta_timestamp, **kwargs)\n","\n","    def dump_data(self, reset:bool, **kwargs:dict[str, str]) -> None:\n","        self._update_df(reset, os.path.join(self.family_path, f'{self.current_meta_timestamp}.csv'), **kwargs)\n","\n","    def get_meta(self) -> pd.DataFrame:\n","        return pd.read_csv(os.path.join(self.family_path, 'meta.csv'))\n","\n","    def get_data(self, timestamp:str) -> pd.DataFrame:\n","        return pd.read_csv(os.path.join(self.family_path, f'{timestamp}.csv'))\n","\n","    def _get_family_path(self, family:str) -> str:\n","        current_folder_index = sum(file_name.startswith(family) for file_name in os.listdir(DRIVE_LOGS_DIR))\n","        family_path = os.path.join(DRIVE_LOGS_DIR, f'{family}_{current_folder_index:04}')\n","        os.mkdir(family_path)\n","        return family_path\n","\n","    def _update_df(self, add_row:bool, path:str, **kwargs:dict[str, str]) -> None:\n","        if os.path.exists(path):\n","            df = pd.read_csv(path, dtype=str)\n","        else:\n","            df = pd.DataFrame(dtype=str)\n","        index = df.shape[0] + add_row - 1\n","        for key, value in kwargs.items():\n","            df.loc[index, key] = str(value)\n","        df.to_csv(path, index=False)"],"metadata":{"id":"wwMgTB3jhfRL","executionInfo":{"status":"ok","timestamp":1758534068839,"user_tz":-120,"elapsed":8,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"tF1kdyWQmEOy","executionInfo":{"status":"ok","timestamp":1758534072517,"user_tz":-120,"elapsed":7,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"outputs":[],"source":["def symbols_to_images(vlm_indices:torch.Tensor, board_dim:int, symbol_size:int, symbols:torch.Tensor, labels:torch.Tensor) -> tuple[list[Image.Image], list[str]]:\n","\n","    n_blocks = int(board_dim ** 0.5)\n","    padding = int(0.2 * symbol_size)\n","    margin = 20\n","\n","    padded_symbol_size = symbol_size + 2 * padding\n","    images = torch.zeros((vlm_indices.shape[0], board_dim * padded_symbol_size, board_dim * padded_symbol_size), dtype=torch.uint8)\n","    for n, vlm_index in enumerate(vlm_indices):\n","        for i in range(board_dim):\n","            for j in range(board_dim):\n","                images[n, i * symbol_size + (2 * i + 1) * padding:(i + 1) * symbol_size + (2 * i + 1) * padding, j * symbol_size + (2 * j + 1) * padding:(j + 1) * symbol_size + (2 * j + 1) * padding] = symbols[vlm_index, i, j, :, :]\n","        for l in range(n_blocks + 1):\n","            images[n, min(board_dim * padded_symbol_size - 1,  l * padded_symbol_size * n_blocks), :] = 255\n","            images[n, :, min(board_dim * padded_symbol_size - 1,  l * padded_symbol_size * n_blocks)] = 255\n","\n","    pil_image_list = list[Image.Image]()\n","    base64_image_list = list[str]()\n","    for n, vlm_index in enumerate(vlm_indices):\n","        raw_pil_image = Image.fromarray(images[n].numpy(), mode='L')\n","        pil_image = Image.new('L', (raw_pil_image.width, raw_pil_image.height + margin), 0)\n","        pil_image.paste(raw_pil_image, (0, 0))\n","        text = f'Label: {labels[vlm_index].item()}'\n","        draw = ImageDraw.Draw(pil_image)\n","        font = ImageFont.load_default(12)\n","        bbox = draw.textbbox((0, 0), text, font=font)\n","        text_width = bbox[2] - bbox[0]\n","        text_height = bbox[3] - bbox[1]\n","        text_x = (pil_image.width - text_width) // 2\n","        text_y = raw_pil_image.height + (margin - text_height) // 2\n","        draw.text((text_x, text_y), text, fill=255, font=font)\n","        buffer = io.BytesIO()\n","        pil_image.save(buffer, format='PNG')\n","        buffer.seek(0)\n","        pil_image_list.append(pil_image)\n","        base64_image_list.append(base64.b64encode(buffer.read()).decode('utf-8'))\n","\n","    return pil_image_list, base64_image_list"]},{"cell_type":"code","source":["def get_loss(truth_values:torch.Tensor, labels:torch.Tensor) -> torch.Tensor:\n","    '''\n","    inputs:\n","        truth_values: Tensor[B][device][torch.float32]\n","        labels: Tensor[B][cpu][torch.bool]\n","    outputs:\n","        loss: Tensor[1][device][torch.float32]\n","    footnotes:\n","        B: batch size, i.e, `batch_size`\n","    '''\n","    labels = labels.to(truth_values.device).to(torch.float32)\n","    loss = (labels + (1.0 - 2.0 * labels) * truth_values).mean()\n","    return loss\n","\n","def get_auc(truth_values:torch.Tensor, labels:torch.Tensor) -> float:\n","    '''\n","    inputs:\n","        truth_values: Tensor[B][device][torch.float32]\n","        labels: Tensor[B][cpu][torch.bool]\n","    outputs:\n","        auc: Number[float]\n","    footnotes:\n","        B: batch size, i.e, `batch_size`\n","    '''\n","    labels = labels.to(torch.float32)\n","    try:\n","        auc = roc_auc_score(labels.detach().numpy(), truth_values.detach().cpu().numpy())\n","    except:\n","        auc = 0.5\n","    return auc\n","\n","def get_accuracy(truth_values:torch.Tensor, labels:torch.Tensor, threshold:float=0.5) -> float:\n","    '''\n","    inputs:\n","        truth_values: Tensor[B][device][torch.float32]\n","        labels: Tensor[B][cpu][torch.bool]\n","    outputs:\n","        accuracy: Number[float]\n","    footnotes:\n","        B: batch size, i.e, `batch_size`\n","    '''\n","    labels = labels.to(truth_values.device)\n","    predictions = truth_values > threshold\n","    accuracy = (predictions == labels).to(torch.float32).mean().item()\n","    return accuracy"],"metadata":{"id":"r461vhYc2Jnm","executionInfo":{"status":"ok","timestamp":1758534074296,"user_tz":-120,"elapsed":5,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def main(dataset_name:str, fol_rule:str, learning_rate:float, batch_size:int, n_epochs:int, power_mean_scheduler:Callable[[int], int], visual_encoder:torch.nn.Module, observation_delay:int=100, observation_patience:int=40, logger:Logger=None, verbose:bool=True) -> tuple[float, float, float]:\n","\n","    # Reseting the Logger\n","    if logger is not None:\n","        logger.dump_meta(\n","            reset=False,\n","            dataset_name=dataset_name,\n","            fol_rule=fol_rule,\n","            learning_rate=learning_rate,\n","            batch_size=batch_size,\n","            n_epochs=n_epochs,\n","            observation_delay=observation_delay,\n","            observation_patience=observation_patience,\n","            power_mean_scheduler=power_mean_scheduler.__name__,\n","            visual_encoder=visual_encoder.__class__.__name__,\n","            **{f'_{key}': value for key, value in visual_encoder.__dict__.items() if not key.startswith('_')}\n","        )\n","\n","    # Object Definition\n","    ## Dataset\n","    dataset_path = f'content/data/{dataset_name}.pt'\n","    data_dict:dict[str, dict[str, torch.Tensor]] = torch.load(dataset_path)\n","    train_symbols = data_dict['train']['symbols'] # Tensor[train_size, board_dim, board_dim, symbol_size, symbol_size][cpu][torch.uint8]\n","    train_labels = data_dict['train']['labels'] # Tensor[train_size][cpu][torch.bool]\n","    val_symbols = data_dict['val']['symbols'] # Tensor[val_size, board_dim, board_dim, symbol_size, symbol_size][cpu][torch.uint8]\n","    val_labels = data_dict['val']['labels'] # Tensor[val_size][cpu][torch.bool]\n","    test_symbols = data_dict['test']['symbols'] # Tensor[test_size, board_dim, board_dim, symbol_size, symbol_size][cpu][torch.uint8]\n","    test_labels = data_dict['test']['labels'] # Tensor[test_size][cpu][torch.bool]\n","\n","    ## Neural Objects\n","    n_training_steps = train_symbols.shape[0] // batch_size\n","    optimizer = torch.optim.Adam(visual_encoder.parameters(), lr=learning_rate)\n","\n","    ## LTN\n","    ltn_formula = LTNBuilder().make_formula(fol_rule)\n","    ltn_formula.ground(\n","        P_same_row=row_similarity,\n","        P_same_col=col_similarity,\n","        P_same_block=block_similarity,\n","        P_same_loc=loc_similarity,\n","        P_same_value=vector_similarity,\n","        logical_and='goguen',\n","        logical_or='goguen',\n","        implies='reichenbach',\n","        forall='power_mean',\n","        exists='power_mean'\n","    )\n","\n","    # Development\n","    ## Eearly Stopping Parameters\n","    observation_counter = 0\n","    best_epoch = -1\n","    best_model_state = None\n","    best_train_loss = float('inf')\n","    best_train_auc = 0.0\n","    best_train_accuracy = 0.0\n","    best_val_loss = float('inf')\n","    best_val_auc = 0.0\n","    best_val_accuracy = 0.0\n","\n","    ## Development Loop\n","    for epoch in range(n_epochs):\n","\n","        ### Updating the Logger\n","        if logger is not None:\n","            logger.dump_data(\n","                reset=True,\n","                epoch=epoch\n","            )\n","\n","        ### Starting a Progress Bar\n","        if verbose:\n","            pbar = tqdm(total=n_training_steps, desc=f'Epoch {epoch}/{n_epochs}')\n","\n","        ### LTN Scheduling\n","        ltn_formula.schedule(\n","            forall = {'p': power_mean_scheduler(epoch)}\n","        )\n","\n","        ### Training\n","        visual_encoder.train()\n","        train_loss_list = list[float]()\n","        train_auc_list = list[float]()\n","        train_accuracy_list = list[float]()\n","        for step in range(n_training_steps):\n","\n","            embeds, indices = visual_encoder(train_symbols[step * batch_size:(step + 1) * batch_size])\n","            variable = torch.cat((embeds, indices), dim=-1)\n","            truth_values = ltn_formula(\n","                x1=variable,\n","                x2=variable\n","            )\n","\n","            loss = get_loss(truth_values, train_labels[step * batch_size:(step + 1) * batch_size])\n","            auc = get_auc(truth_values, train_labels[step * batch_size:(step + 1) * batch_size])\n","            accuracy = get_accuracy(truth_values, train_labels[step * batch_size:(step + 1) * batch_size])\n","\n","            torch.nn.utils.clip_grad_norm_(visual_encoder.parameters(), max_norm=1.0)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss_list.append(loss.item())\n","            train_auc_list.append(auc)\n","            train_accuracy_list.append(accuracy)\n","            train_loss = sum(train_loss_list) / len(train_loss_list)\n","            train_auc = sum(train_auc_list) / len(train_auc_list)\n","            train_accuracy = sum(train_accuracy_list) / len(train_accuracy_list)\n","\n","            if verbose:\n","                pbar.set_postfix_str(f'Train (Loss: {train_loss:.4f}, AUC: {train_auc:.4f}), Accuracy: {train_accuracy:.2%}')\n","                pbar.update()\n","\n","        ### Updating the Logger\n","        if logger is not None:\n","            logger.dump_data(\n","                reset=False,\n","                train_loss=train_loss,\n","                train_auc=train_auc,\n","                train_accuracy=train_accuracy\n","            )\n","\n","        ### Validation\n","        visual_encoder.eval()\n","        with torch.no_grad():\n","\n","            embeds, indices = visual_encoder(val_symbols)\n","            variable = torch.cat((embeds, indices), dim=-1)\n","            truth_values = ltn_formula(\n","                x1=variable,\n","                x2=variable\n","            )\n","\n","            val_loss = get_loss(truth_values, val_labels).item()\n","            val_auc = get_auc(truth_values, val_labels)\n","            val_accuracy = get_accuracy(truth_values, val_labels)\n","\n","            if verbose:\n","                pbar.set_postfix_str(f'Train (Loss: {train_loss:.4f}, AUC: {train_auc:.4f}, Accuracy: {train_accuracy:.2%}), Val (Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, Acuuracy: {val_accuracy:.2%})')\n","\n","        ### Updating the Logger\n","        if logger is not None:\n","            logger.dump_data(\n","                reset=False,\n","                val_loss=val_loss,\n","                val_auc=val_auc,\n","                val_accuracy=val_accuracy\n","            )\n","\n","        ### Closing the Progress Bar\n","        if verbose:\n","            pbar.close()\n","\n","        ## Early Stopping\n","        if epoch >= observation_delay:\n","            if val_loss < best_val_loss:\n","                best_epoch = epoch\n","                best_model_state = visual_encoder.state_dict()\n","                best_train_loss = train_loss\n","                best_train_auc = train_auc\n","                best_train_accuracy = train_accuracy\n","                best_val_loss = val_loss\n","                best_val_auc = val_auc\n","                best_val_accuracy = val_accuracy\n","                observation_counter = 0\n","            else:\n","                observation_counter += 1\n","            if observation_counter >= observation_patience:\n","                break\n","\n","    ## Evaluation\n","    if best_model_state is not None:\n","        visual_encoder.load_state_dict(best_model_state)\n","    visual_encoder.eval()\n","    with torch.no_grad():\n","\n","        embeds, indices = visual_encoder(test_symbols)\n","        variable = torch.cat((embeds, indices), dim=-1)\n","        truth_values = ltn_formula(\n","            x1=variable,\n","            x2=variable\n","        )\n","\n","        test_loss = get_loss(truth_values, test_labels).item()\n","        test_auc = get_auc(truth_values, test_labels)\n","        test_accuracy = get_accuracy(truth_values, test_labels)\n","\n","    ## Finalizing the Logger\n","    if logger is not None:\n","        logger.dump_meta(\n","            reset=False,\n","            stop_epoch=epoch,\n","            best_epoch=best_epoch,\n","            best_train_loss=best_train_loss,\n","            best_train_auc=best_train_auc,\n","            best_train_accuracy=best_train_accuracy,\n","            best_val_loss=best_val_loss,\n","            best_val_auc=best_val_auc,\n","            best_val_accuracy=best_val_accuracy,\n","            test_loss=test_loss,\n","            test_auc=test_auc,\n","            test_accuracy=test_accuracy\n","        )\n","\n","    ## Results\n","    return test_loss, test_auc, test_accuracy"],"metadata":{"id":"Sag6ZIUu2Fqd","executionInfo":{"status":"ok","timestamp":1758534078096,"user_tz":-120,"elapsed":11,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Hyperparameter Tuning"],"metadata":{"id":"NiFpgXVAgGoT"}},{"cell_type":"code","source":["dataset_name_list = [\n","    'mnist_4x4_split_11',\n","    'emnist_4x4_split_11',\n","    'kmnist_4x4_split_11',\n","    'fmnist_4x4_split_11'\n","]\n","fol_rule = 'forall x1, x2 P_same_value(x1, x2) implies P_same_loc(x1, x2) | (!P_same_row(x1, x2) & !P_same_col(x1, x2) & !P_same_block(x1, x2))'\n","\n","grid = list(itertools.product(\n","    [(8, 16), (16, 32), (32, 64)],  # cnn_dims\n","    [(4, 4)],                       # kernel_dims\n","    [(4,), (64,), (64, 4)],         # embed_dims\n","    [0.1, 0.2],                     # drop_prob\n","    [False, True]                   # use_softmax\n","))\n","\n","logger = Logger('hyperparameter_tuning')\n","pbar = tqdm(total=len(grid) * len(dataset_name_list), desc='Hyperparameter Tuning')\n","best_avg_test_loss = float('inf')\n","best_params = dict[str, Any]()\n","for cnn_dims, kernel_dims, embed_dims, drop_prob, use_softmax in grid:\n","\n","    test_loss_list = list[float]()\n","    for dataset_name in dataset_name_list:\n","        logger.dump_meta(reset=True)\n","        test_loss, _, _  = main(\n","            dataset_name=dataset_name,\n","            fol_rule=fol_rule,\n","            learning_rate=0.0005,\n","            batch_size=8,\n","            n_epochs=400,\n","            visual_encoder=EncoderNet(\n","                device='cuda' if torch.cuda.is_available() else 'cpu',\n","                symbol_size=28,\n","                board_dim=4,\n","                cnn_dims=cnn_dims,\n","                kernel_dims=kernel_dims,\n","                embed_dims=embed_dims,\n","                drop_prob=drop_prob,\n","                use_softmax=use_softmax\n","            ),\n","            power_mean_scheduler=old_power_mean_scheduler,\n","            logger=logger,\n","            verbose=False\n","        )\n","        test_loss_list.append(test_loss)\n","        pbar.update()\n","\n","    avg_test_loss = sum(test_loss_list) / len(test_loss_list)\n","    if avg_test_loss < best_avg_test_loss:\n","        best_avg_test_loss = avg_test_loss\n","        best_params = {\n","            'cnn_dims': cnn_dims,\n","            'kernel_dims': kernel_dims,\n","            'embed_dims': embed_dims,\n","            'drop_prob': drop_prob,\n","            'use_softmax': use_softmax\n","        }\n","\n","print('Best Hyperparameters:')\n","print(f'  Best average test loss: {best_avg_test_loss}')\n","for key, value in best_params.items():\n","    print(f'  {key}: {value}')"],"metadata":{"id":"gphfbzkwjV7E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9COhg-qkoSx"},"source":["## Rule Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2Tfvvnl3al5"},"outputs":[],"source":["ltn_builder = LTNBuilder()\n","client = Groq(api_key=GROQ_API_KEY)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYzeJOvfoiOf"},"outputs":[],"source":["## Logger\n","logger = Logger(\n","    dataset_name = 'fmnist_4x4_split_11',\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n","    max_iterations = 20,\n","    vlm_load = 3,\n","    termination_auc = 0.95,\n","    learning_rate = 0.001,\n","    batch_size = 64,\n","    n_epochs = 400,\n","    symbol_size = 28,\n","    cnn_hidden_dims = (16, 32, 64),\n","    embed_dim = 128,\n","    drop_prob = 0.1,\n","    observation_delay = 100,\n","    observation_patience = 50\n",")\n","\n","## Dataset Handling\n","dataset_path = os.path.join(DATASETS_DIR, f'{logger.dataset_name}.pt')\n","data_dict:dict[str, dict[str, torch.Tensor]] = torch.load(dataset_path)\n","\n","train_dict = data_dict['train']\n","train_symbols = train_dict['symbols']\n","train_digits = train_dict['digits']\n","train_labels = train_dict['labels']\n","\n","val_dict = data_dict['val']\n","val_symbols = val_dict['symbols']\n","val_digits = val_dict['digits']\n","val_labels = val_dict['labels']\n","\n","test_dict = data_dict['test']\n","test_symbols = test_dict['symbols']\n","test_digits = test_dict['digits']\n","test_labels = test_dict['labels']\n","\n","n_images, board_dim, _, symbol_size, _ = train_symbols.shape\n","\n","## Initialization\n","observation_counter = 0\n","best_epoch = -1\n","best_train_loss = float('inf')\n","best_train_auc = 0.0\n","best_val_loss = float('inf')\n","best_val_auc = 0.0\n","best_model_state = torch.nn.Module()\n","history_list = list[tuple[str, str, float]]()\n","\n","# Loop\n","for trial in range(logger.max_iterations):\n","\n","    ## Trial\n","    logger.dump('trial', trial, True)\n","\n","    # VLM\n","    vlm_start_time = time.time()\n","    system_role = (\n","        'You are a helpful assistant that can extract the First-Order Logic (FOL) rule from images.'\n","        '\\nTHE GRAMMAR OF FOL:'\n","        '\\n- Constants: Not allowed in the rule.'\n","        '\\n- Variables: Your options are `x1`, `x2`, ..., which represent visual objects.'\n","        '\\n- Functions: Not allowed in the rule.'\n","        '\\n- Predicates: Your options are `P_same_row`, `P_same_col`, `P_same_block`, `P_same_loc`, and `P_same_value`.'\n","        '\\n- To compare variables, only use predicates.'\n","        '\\n- The symbols used for logical AND, OR, and NOT are respectively `&`, `|`, and `!`'\n","        '\\n- The symbols used for implication and equivalence are respectively `implies` and `iff`.'\n","        '\\n- The symbols used for universal and existential quantifiers are respectively `forall` and `exists`.'\n","        '\\n- Use parentheses for preserving operation precedence.'\n","        '\\nWHAT YOU MUST CONSIDER:'\n","        '\\n- Use your own knowledge to analyze and deeply think about the images provided as your reference.'\n","        '\\n- All the images must follow the same rule that you extract.'\n","        '\\n- The rule applies to the visual objects within each image.'\n","        '\\n- The visual objects may represent numbers rather than what they really are.'\n","        '\\n- At the end of your chain of thought, put the extracted rule in the following template:'\n","        '\\n  EXTRACTED_RULE: \"the rule you extracted\"'\n","    )\n","    if len(history_list) > 0:\n","        system_role += '\\nHISTORY OF PREVIOUS TRIALS:'\n","        for old_trial, (error_message, extracted_fol_rule, test_auc) in enumerate(history_list):\n","            system_role += f'\\n- Trial {old_trial+1} -> '\n","            if error_message != '':\n","                system_role += f'error: \"{error_message}\"'\n","            else:\n","                system_role += f'extracted rule: \"{extracted_fol_rule}\", average auc: {test_auc}'\n","        if test_auc < logger.termination_auc:\n","            system_role += '\\nIMPORTANT LESSON FROM HISTORY:'\n","            if all(extracted_fol_rule == '' for _, extracted_fol_rule, _ in history_list):\n","                system_role += '- Pay attention to the the instructions!'\n","            else:\n","                system_role += '- The next FOL rule must be an improved version of the above!'\n","    logger.dump('system_role', system_role, False)\n","\n","    user_role = [{'type': 'text', 'text': 'These are the reference images:'}]\n","    vlm_indices = torch.randperm(n_images)[:logger.vlm_load]\n","    _, base64_image_list = symbols_to_images(vlm_indices, board_dim, symbol_size, train_symbols, train_labels)\n","    for base64_image in base64_image_list:\n","        user_role.append({'type': 'image_url', 'image_url': {'url': f'data:image/png;base64,{base64_image}'}})\n","\n","    chat_completion = client.chat.completions.create(\n","        messages=[\n","            {'role': 'system', 'content': system_role},\n","            {'role': 'user', 'content': user_role}\n","        ],\n","        model='meta-llama/llama-4-maverick-17b-128e-instruct',\n","    )\n","    response = chat_completion.choices[0].message.content\n","    vlm_end_time = time.time()\n","    logger.dump('response', response, False)\n","\n","    # D-LTN\n","    dltn_start_time = time.time()\n","    try:\n","        extracted_fol_rule:str = re.search(r'EXTRACTED_RULE\\s*:\\s*\"([^\"]*)\"', response).group(1)\n","        logger.dump('extracted_fol_rule', extracted_fol_rule, True)\n","    except:\n","        exception_message = 'No FOL rule could not be extracted from your response!'\n","        history_list.append((exception_message, '', 0.0))\n","        logger.dump('exception_message', exception_message, True)\n","        continue\n","\n","    try:\n","        ltn_formula = ltn_builder.make_formula(extracted_fol_rule)\n","    except:\n","        exception_message = 'No FOL rule could not be parsed from your response!'\n","        history_list.append((exception_message, '', 0.0))\n","        logger.dump('exception_message', exception_message, True)\n","        continue\n","\n","    try:\n","        ltn_formula.ground(\n","            P_same_row=row_similarity,\n","            P_same_col=col_similarity,\n","            P_same_block=block_similarity,\n","            P_same_loc=loc_similarity,\n","            P_same_value=vector_similarity,\n","            logical_and='goguen',\n","            logical_or='goguen',\n","            implies='reichenbach',\n","            forall='power_mean',\n","            exists='power_mean'\n","        )\n","        ltn_formula(\n","            x1=torch.rand(1, 4, 4, device=logger.device),\n","            x2=torch.rand(1, 4, 4, device=logger.device)\n","        )\n","    except:\n","        exception_message = 'There were problems with your groundings!'\n","        history_list.append((exception_message, '', 0.0))\n","        logger.dump('exception_message', exception_message, True)\n","        continue\n","    dltn_end_time = time.time()\n","\n","    ## Visual Encoder\n","    visual_encoder = VisualEncoder(\n","        device = logger.device,\n","        symbol_size = logger.symbol_size,\n","        cnn_hidden_dims = logger.cnn_hidden_dims,\n","        embed_dim = logger.embed_dim,\n","        drop_prob = logger.drop_prob\n","    )\n","    optimizer = torch.optim.Adam(visual_encoder.parameters(), lr=logger.learning_rate)\n","\n","    ## Development\n","    dev_start_time = time.time()\n","    for epoch in range(logger.n_epochs):\n","\n","        if epoch < 20:\n","            power_mean_p = 1\n","        elif epoch < 120:\n","            power_mean_p = 2\n","        elif epoch < 140:\n","            power_mean_p = 4\n","        elif epoch < 170:\n","            power_mean_p = 6\n","        elif epoch < 200:\n","            power_mean_p = 8\n","        else:\n","            power_mean_p = 10\n","        ltn_formula.schedule(\n","            forall = {'p': power_mean_p}\n","        )\n","\n","        visual_encoder.train()\n","        train_loss_list = list[float]()\n","        train_auc_list = list[float]()\n","        for step in range(train_size // logger.batch_size):\n","            x_train = visual_encoder(train_symbols[step * logger.batch_size:(step + 1) * logger.batch_size])\n","            truth_values = ltn_formula(\n","                x1=x_train,\n","                x2=x_train\n","            )\n","            labels = train_labels[step * logger.batch_size:(step + 1) * logger.batch_size].to(logger.device).to(torch.float32)\n","            loss = torch.nn.functional.binary_cross_entropy(truth_values, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            try:\n","                auc = roc_auc_score(labels.detach().cpu().numpy(), truth_values.detach().cpu().numpy())\n","            except ValueError:\n","                auc = 0.5\n","            train_loss_list.append(loss.item())\n","            train_auc_list.append(auc)\n","        train_loss = sum(train_loss_list) / len(train_loss_list)\n","        train_auc = sum(train_auc_list) / len(train_auc_list)\n","\n","        visual_encoder.eval()\n","        with torch.no_grad():\n","            x_val = visual_encoder(val_symbols)\n","            truth_values = ltn_formula(\n","                x1=x_val,\n","                x2=x_val\n","            )\n","            labels = val_labels.to(logger.device).to(torch.float32)\n","            loss = torch.nn.functional.binary_cross_entropy(truth_values, labels)\n","            try:\n","                auc = roc_auc_score(labels.detach().cpu().numpy(), truth_values.detach().cpu().numpy())\n","            except ValueError:\n","                auc = 0.5\n","            val_loss = loss.item()\n","            val_auc = auc\n","\n","        if epoch >= logger.observation_delay:\n","            if val_loss < best_val_loss:\n","                best_epoch = epoch\n","                best_train_loss = train_loss\n","                best_train_auc = train_auc\n","                best_val_loss = val_loss\n","                best_val_auc = val_auc\n","                best_model_state = visual_encoder.state_dict()\n","                observation_counter = 0\n","            else:\n","                observation_counter += 1\n","            if observation_counter >= logger.observation_patience:\n","                break\n","    dev_end_time = time.time()\n","    logger.dump('stop_epoch', epoch, True)\n","    logger.dump('best_epoch', best_epoch, True)\n","    logger.dump('train_loss', train_loss, True)\n","    logger.dump('val_loss', val_loss, True)\n","    logger.dump('train_auc', train_auc, True)\n","    logger.dump('val_auc', val_auc, True)\n","\n","    ## Evaluation\n","    visual_encoder.load_state_dict(best_model_state)\n","    visual_encoder.eval()\n","    with torch.no_grad():\n","        x_test = visual_encoder(test_symbols)\n","        truth_values = ltn_formula(\n","            x1=x_test,\n","            x2=x_test\n","        )\n","        labels = test_labels.to(logger.device).to(torch.float32)\n","        loss = torch.nn.functional.binary_cross_entropy(truth_values, labels)\n","        try:\n","            auc = roc_auc_score(labels.detach().cpu().numpy(), truth_values.detach().cpu().numpy())\n","        except ValueError:\n","            auc = 0.5\n","        test_loss = loss.item()\n","        test_auc = auc\n","    eval_end_time = time.time()\n","    logger.dump('test_loss', test_loss, True)\n","    logger.dump('test_auc', test_auc, True)\n","\n","    ## Delays\n","    vlm_delay = vlm_end_time - vlm_start_time\n","    dltn_delay = dltn_end_time - dltn_start_time\n","    dev_delay = dev_end_time - dev_start_time\n","    eval_delay = eval_end_time - dev_end_time\n","    logger.dump('vlm_delay', vlm_delay, True)\n","    logger.dump('dltn_delay', dltn_delay, True)\n","    logger.dump('dev_delay', dev_delay, True)\n","    logger.dump('eval_delay', eval_delay, True)\n","\n","    ## Loop Completion\n","    history_list.append(('', extracted_fol_rule, test_auc))\n","    if test_auc >= logger.termination_auc:\n","        break"]},{"cell_type":"markdown","source":["## Experimental Tests"],"metadata":{"id":"DanVEW3cEHec"}},{"cell_type":"code","source":["data_source_lits = [\n","    'mnist_4x4',\n","    'emnist_4x4',\n","    'kmnist_4x4',\n","    'fmnist_4x4'\n","]\n","split_list = [f'{split}:02' for split in range(1, 11)]\n","fol_rule_list = [\n","    'forall x1, x2 !P_same_loc(x1, x2) & (P_same_row(x1, x2) | P_same_col(x1, x2) | P_same_block(x1, x2)) implies !P_same_value(x1, x2)',\n","    'forall x1 forall x2 (P_same_row(x1, x2) | P_same_col(x1, x2) | P_same_block(x1, x2)) & !P_same_loc(x1, x2) implies !P_same_value(x1, x2)',\n","    'forall x1, x2 (!P_same_loc(x1, x2) & P_same_value(x1, x2)) implies (!P_same_row(x1, x2) & !P_same_col(x1, x2) & !P_same_block(x1, x2))'\n","]\n","\n","logger = Logger('experimental_tests')\n","pbar = tqdm(total=len(data_source_lits) * len(split_list) * len(fol_rule_list), desc='Experimental Tests')\n","for data_source_name in data_source_lits:\n","    for split in split_list:\n","        for fol_rule in fol_rule_list:\n","            logger.dump_meta(reset=True)\n","            main(\n","                dataset_name=f'{data_source_name}_split_{split}',\n","                fol_rule=fol_rule,\n","                learning_rate=0.0005,\n","                batch_size=32,\n","                n_epochs=400,\n","                visual_encoder=EncoderNet(\n","                    device='cuda' if torch.cuda.is_available() else 'cpu',\n","                    symbol_size=28,\n","                    board_dim=4,\n","                    cnn_dims=(32, 64),\n","                    kernel_dims=(4, 4),\n","                    embed_dims=(64,),\n","                    drop_prob=0.2,\n","                    use_softmax=True\n","                ),\n","                power_mean_scheduler=new_power_mean_scheduler,\n","                logger=logger,\n","                verbose=False\n","            )\n","            pbar.update()"],"metadata":{"id":"pYvycESwEGfs","executionInfo":{"status":"ok","timestamp":1752428721753,"user_tz":-120,"elapsed":2,"user":{"displayName":"Homie Magenta","userId":"12381401514309698684"}},"outputId":"32d3f1c9-f466-4b23-99ba-db04152705ba","colab":{"base_uri":"https://localhost:8080/","height":35,"referenced_widgets":["5c0abb7d997148e69fdef56095a6c9d2"]}},"execution_count":null,"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c0abb7d997148e69fdef56095a6c9d2","version_major":2,"version_minor":0},"text/plain":["Experimental Tests:   0%|          | 0/120 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"markdown","metadata":{"id":"cEooKbC22xng"},"source":["## Rule Test"]},{"cell_type":"code","source":["dataset_name = 'mnist_4x4_split_08'\n","# dataset_name = 'kmnist_4x4_split_05'\n","\n","# fol_rule = 'forall x1, x2 !P_same_loc(x1, x2) & (P_same_row(x1, x2) | P_same_col(x1, x2) | P_same_block(x1, x2)) implies !P_same_value(x1, x2)'\n","# fol_rule = 'forall x1 forall x2 (P_same_row(x1, x2) | P_same_col(x1, x2) | P_same_block(x1, x2)) & !P_same_loc(x1, x2) implies !P_same_value(x1, x2)'\n","fol_rule = 'forall x1, x2 (!P_same_loc(x1, x2) & P_same_value(x1, x2)) implies (!P_same_row(x1, x2) & !P_same_col(x1, x2) & !P_same_block(x1, x2))'\n","\n","visual_encoder = SudokuNet(\n","    device='cuda' if torch.cuda.is_available() else 'cpu',\n","    symbol_size=28,\n","    board_dim=4,\n","    drop_prob=0.2,\n","    use_softmax=False\n",")\n","# visual_encoder = EncoderNet(\n","#     device='cuda' if torch.cuda.is_available() else 'cpu',\n","#     symbol_size=28,\n","#     board_dim=4,\n","#     cnn_dims=(16, 32),\n","#     kernel_dims=(4, 4),\n","#     embed_dims=(64, 16),\n","#     drop_prob=0.5,\n","#     use_softmax=True\n","# )\n","\n","# power_mean_scheduler = original_power_mean_scheduler\n","power_mean_scheduler = new_power_mean_scheduler\n","\n","logs = main(\n","    dataset_name=dataset_name,\n","    fol_rule=fol_rule,\n","    learning_rate=0.0005,\n","    batch_size=8,\n","    n_epochs=400,\n","    visual_encoder=visual_encoder,\n","    power_mean_scheduler=new_power_mean_scheduler,\n","    verbose=True\n",")\n","print(logs.meta_sr)"],"metadata":{"id":"T3l7X3CC0IxF"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["d9COhg-qkoSx"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}